{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    # здесь и далее заменил np.float на np.float64, тк иначе ругается интерпритатор\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float64) / 255.0 \n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float64) / 255.0\n",
    "\n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "assert np.isclose(probs[0], 0.0)\n",
    "# print(probs[2])\n",
    "# assert np.isclose(probs[2], 1.0, rtol=0.0001)\n",
    "\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO implement cross-entropy\n",
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)\n",
    "\n",
    "# probs = np.arange(15).reshape(3,5)\n",
    "# print(probs)\n",
    "# loss = linear_classifer.cross_entropy_loss(probs, np.array([4, 0, 2]))\n",
    "# assert np.isclose(loss, -np.sum(np.log([4, 5, 12]))/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращать и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(float)\n",
    "# target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(int)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(int)  # Все функции требуют: 'target_index: np array of int' => поменял размерность\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(float)\n",
    "target_index = np.ones(batch_size, dtype=int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.485461\n",
      "Epoch 1, loss: 2.356457\n",
      "Epoch 2, loss: 2.318115\n",
      "Epoch 3, loss: 2.306686\n",
      "Epoch 4, loss: 2.303318\n",
      "Epoch 5, loss: 2.302292\n",
      "Epoch 6, loss: 2.301992\n",
      "Epoch 7, loss: 2.301901\n",
      "Epoch 8, loss: 2.301876\n",
      "Epoch 9, loss: 2.301865\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4c1193ff10>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.125\n",
      "Accuracy after training for 100 epochs:  0.127\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.254000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [5e-3, 3e-3, 1e-3, 5e-4, 3e-4, 1e-4, 5e-5, 3e-5, 1e-5, 5e-6, 3e-6, 1e-6]\n",
    "reg_strengths = [5e-3, 3e-3, 1e-3, 5e-4, 3e-4, 1e-4, 5e-5, 3e-5, 1e-5, 5e-6, 3e-6, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for rate in learning_rates:\n",
    "    for strength in reg_strengths:\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=rate, batch_size=batch_size, reg=strength)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "\n",
    "        if best_val_accuracy < accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.219000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
