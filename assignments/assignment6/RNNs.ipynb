{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda is here!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Cuda is here!\")\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /home/kkorzhanevskii/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/kkorzhanevskii/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Пример размеченного предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words count in train set: 739769\n",
      "Words count in val set: 130954\n",
      "Words count in test set: 290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
    "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
    "print('Words count in test set:', sum(len(sent) for sent in test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'.', 'ADP', 'X', 'PRT', 'VERB', 'CONJ', 'NUM', 'NOUN', 'DET', 'PRON', 'ADV', 'ADJ'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, _ in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for _, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of unigram tagger = 92.62%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NN')\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.accuracy(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger = 93.42%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.accuracy(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger = 23.33%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.accuracy(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind_):\n",
    "    x = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind_[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    x, y = data\n",
    "\n",
    "    n_samples = len(x)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(x[ind]) for ind in batch_indices)\n",
    "        x_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            x_batch[:len(x[sample_ind]), batch_ind] = x[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "((32, 4), (32, 4))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.linear = nn.Linear(lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out, _ = self.lstm(self.embedding(inputs))\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1304)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "_, indices = torch.max(logits, dim=2)\n",
    "correct_samples = torch.sum((indices == y_batch) & (y_batch != 0))\n",
    "total_samples = torch.sum(y_batch != 0)\n",
    "accuracy = correct_samples / total_samples\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(logits.transpose(2, 1), y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "\n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "\n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "\n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (x_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                x_batch, y_batch = LongTensor(x_batch), LongTensor(y_batch)\n",
    "                logits = model(x_batch)\n",
    "\n",
    "                loss = criterion(logits.transpose(2, 1), y_batch)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                indices = torch.argmax(logits, dim=-1)\n",
    "                cur_correct_count, cur_sum_count = torch.sum((indices == y_batch) & (y_batch != 0)), torch.sum(y_batch != 0)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "\n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
    "        val_data=None, val_batch_size=None, train_hstr=None, val_hstr=None):\n",
    "\n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "\n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        if not train_hstr is None:\n",
    "            train_hstr.append(train_acc.cpu())\n",
    "\n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
    "            if not val_hstr is None:\n",
    "                val_hstr.append(val_acc.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pqfbeh1ltEYa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.32296, Accuracy = 90.04%:  84%|████████▍ | 483/572 [00:03<00:00, 134.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [44]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mCrossEntropyLoss(ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m      7\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters())\n\u001B[0;32m----> 9\u001B[0m \u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs_count\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m512\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36mfit\u001B[0;34m(model, criterion, optimizer, train_data, epochs_count, batch_size, val_data, val_batch_size, train_hstr, val_hstr)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs_count):\n\u001B[1;32m     56\u001B[0m     name_prefix \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m / \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m] \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m, epochs_count)\n\u001B[0;32m---> 57\u001B[0m     train_loss, train_acc \u001B[38;5;241m=\u001B[39m \u001B[43mdo_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname_prefix\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mTrain:\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m train_hstr:\n\u001B[1;32m     59\u001B[0m         train_hstr\u001B[38;5;241m.\u001B[39mappend(train_acc)\n",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36mdo_epoch\u001B[0;34m(model, criterion, data, batch_size, optimizer, name)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tqdm(total\u001B[38;5;241m=\u001B[39mbatches_count) \u001B[38;5;28;01mas\u001B[39;00m progress_bar:\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, (x_batch, y_batch) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(iterate_batches(data, batch_size)):\n\u001B[0;32m---> 19\u001B[0m         x_batch, y_batch \u001B[38;5;241m=\u001B[39m \u001B[43mLongTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_batch\u001B[49m\u001B[43m)\u001B[49m, LongTensor(y_batch)\n\u001B[1;32m     20\u001B[0m         logits \u001B[38;5;241m=\u001B[39m model(x_batch)\n\u001B[1;32m     22\u001B[0m         loss \u001B[38;5;241m=\u001B[39m criterion(logits\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m), y_batch)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 1] Train: Loss = 0.34257, Accuracy = 94.74%: 100%|██████████| 448/448 [00:01<00:00, 390.34it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(model, criterion, None, train_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.55192, Accuracy = 82.59%: 100%|██████████| 572/572 [00:06<00:00, 88.81it/s] \n",
      "[1 / 50]   Val: Loss = 0.26872, Accuracy = 91.32%: 100%|██████████| 13/13 [00:00<00:00, 117.54it/s]\n",
      "[2 / 50] Train: Loss = 0.20366, Accuracy = 93.51%: 100%|██████████| 572/572 [00:06<00:00, 90.28it/s] \n",
      "[2 / 50]   Val: Loss = 0.17456, Accuracy = 94.52%: 100%|██████████| 13/13 [00:00<00:00, 115.30it/s]\n",
      "[3 / 50] Train: Loss = 0.12940, Accuracy = 95.98%: 100%|██████████| 572/572 [00:06<00:00, 88.59it/s] \n",
      "[3 / 50]   Val: Loss = 0.13733, Accuracy = 95.58%: 100%|██████████| 13/13 [00:00<00:00, 111.07it/s]\n",
      "[4 / 50] Train: Loss = 0.08843, Accuracy = 97.30%: 100%|██████████| 572/572 [00:06<00:00, 88.68it/s]\n",
      "[4 / 50]   Val: Loss = 0.12614, Accuracy = 95.86%: 100%|██████████| 13/13 [00:00<00:00, 112.22it/s]\n",
      "[5 / 50] Train: Loss = 0.06117, Accuracy = 98.18%: 100%|██████████| 572/572 [00:06<00:00, 86.25it/s]\n",
      "[5 / 50]   Val: Loss = 0.11634, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 115.29it/s]\n",
      "[6 / 50] Train: Loss = 0.04171, Accuracy = 98.80%: 100%|██████████| 572/572 [00:06<00:00, 87.64it/s]\n",
      "[6 / 50]   Val: Loss = 0.11612, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 111.64it/s]\n",
      "[7 / 50] Train: Loss = 0.02794, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 89.76it/s]\n",
      "[7 / 50]   Val: Loss = 0.11834, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 112.02it/s]\n",
      "[8 / 50] Train: Loss = 0.01831, Accuracy = 99.53%: 100%|██████████| 572/572 [00:06<00:00, 82.55it/s] \n",
      "[8 / 50]   Val: Loss = 0.12376, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 115.69it/s]\n",
      "[9 / 50] Train: Loss = 0.01179, Accuracy = 99.72%: 100%|██████████| 572/572 [00:06<00:00, 89.07it/s]  \n",
      "[9 / 50]   Val: Loss = 0.13079, Accuracy = 96.42%: 100%|██████████| 13/13 [00:00<00:00, 107.36it/s]\n",
      "[10 / 50] Train: Loss = 0.00745, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 85.68it/s] \n",
      "[10 / 50]   Val: Loss = 0.14191, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 106.83it/s]\n",
      "[11 / 50] Train: Loss = 0.00472, Accuracy = 99.92%: 100%|██████████| 572/572 [00:06<00:00, 86.81it/s] \n",
      "[11 / 50]   Val: Loss = 0.15433, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 121.00it/s]\n",
      "[12 / 50] Train: Loss = 0.00301, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 87.63it/s]  \n",
      "[12 / 50]   Val: Loss = 0.15769, Accuracy = 96.41%: 100%|██████████| 13/13 [00:00<00:00, 117.92it/s]\n",
      "[13 / 50] Train: Loss = 0.00179, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 86.30it/s] \n",
      "[13 / 50]   Val: Loss = 0.16433, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 125.94it/s]\n",
      "[14 / 50] Train: Loss = 0.00141, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 86.58it/s] \n",
      "[14 / 50]   Val: Loss = 0.17974, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 124.05it/s]\n",
      "[15 / 50] Train: Loss = 0.00182, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 86.79it/s] \n",
      "[15 / 50]   Val: Loss = 0.19080, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 119.90it/s]\n",
      "[16 / 50] Train: Loss = 0.00279, Accuracy = 99.94%: 100%|██████████| 572/572 [00:06<00:00, 87.68it/s] \n",
      "[16 / 50]   Val: Loss = 0.18914, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 113.56it/s]\n",
      "[17 / 50] Train: Loss = 0.00164, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 90.53it/s]  \n",
      "[17 / 50]   Val: Loss = 0.19079, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 125.34it/s]\n",
      "[18 / 50] Train: Loss = 0.00063, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 89.48it/s]  \n",
      "[18 / 50]   Val: Loss = 0.19622, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 121.58it/s]\n",
      "[19 / 50] Train: Loss = 0.00031, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.98it/s]\n",
      "[19 / 50]   Val: Loss = 0.20341, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 113.27it/s]\n",
      "[20 / 50] Train: Loss = 0.00019, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.94it/s]\n",
      "[20 / 50]   Val: Loss = 0.20455, Accuracy = 96.44%: 100%|██████████| 13/13 [00:00<00:00, 119.78it/s]\n",
      "[21 / 50] Train: Loss = 0.00030, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.76it/s]\n",
      "[21 / 50]   Val: Loss = 0.22645, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 118.95it/s]\n",
      "[22 / 50] Train: Loss = 0.00517, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 84.92it/s] \n",
      "[22 / 50]   Val: Loss = 0.20695, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 121.27it/s]\n",
      "[23 / 50] Train: Loss = 0.00112, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 90.54it/s]  \n",
      "[23 / 50]   Val: Loss = 0.21629, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 118.91it/s]\n",
      "[24 / 50] Train: Loss = 0.00026, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.44it/s] \n",
      "[24 / 50]   Val: Loss = 0.21025, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 115.16it/s]\n",
      "[25 / 50] Train: Loss = 0.00013, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.66it/s]\n",
      "[25 / 50]   Val: Loss = 0.21123, Accuracy = 96.54%: 100%|██████████| 13/13 [00:00<00:00, 114.27it/s]\n",
      "[26 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.84it/s] \n",
      "[26 / 50]   Val: Loss = 0.21487, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 129.86it/s]\n",
      "[27 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 88.94it/s] \n",
      "[27 / 50]   Val: Loss = 0.21247, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 119.46it/s]\n",
      "[28 / 50] Train: Loss = 0.00011, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.13it/s]\n",
      "[28 / 50]   Val: Loss = 0.21731, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 126.07it/s]\n",
      "[29 / 50] Train: Loss = 0.00442, Accuracy = 99.86%: 100%|██████████| 572/572 [00:06<00:00, 88.17it/s]  \n",
      "[29 / 50]   Val: Loss = 0.21805, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 120.38it/s]\n",
      "[30 / 50] Train: Loss = 0.00128, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 87.73it/s] \n",
      "[30 / 50]   Val: Loss = 0.21973, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 119.03it/s]\n",
      "[31 / 50] Train: Loss = 0.00037, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 83.17it/s] \n",
      "[31 / 50]   Val: Loss = 0.21871, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 125.63it/s]\n",
      "[32 / 50] Train: Loss = 0.00011, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.50it/s]\n",
      "[32 / 50]   Val: Loss = 0.21737, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 122.86it/s]\n",
      "[33 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.03it/s]\n",
      "[33 / 50]   Val: Loss = 0.21836, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 117.11it/s]\n",
      "[34 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.41it/s] \n",
      "[34 / 50]   Val: Loss = 0.22178, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 112.37it/s]\n",
      "[35 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.85it/s]\n",
      "[35 / 50]   Val: Loss = 0.22161, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 119.73it/s]\n",
      "[36 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.98it/s] \n",
      "[36 / 50]   Val: Loss = 0.22517, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 116.21it/s]\n",
      "[37 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 92.80it/s] \n",
      "[37 / 50]   Val: Loss = 0.22964, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 121.00it/s]\n",
      "[38 / 50] Train: Loss = 0.00464, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 86.30it/s] \n",
      "[38 / 50]   Val: Loss = 0.24300, Accuracy = 96.40%: 100%|██████████| 13/13 [00:00<00:00, 129.25it/s]\n",
      "[39 / 50] Train: Loss = 0.00106, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 88.45it/s] \n",
      "[39 / 50]   Val: Loss = 0.23185, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 121.73it/s]\n",
      "[40 / 50] Train: Loss = 0.00021, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.98it/s]\n",
      "[40 / 50]   Val: Loss = 0.22941, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 129.37it/s]\n",
      "[41 / 50] Train: Loss = 0.00007, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.54it/s]\n",
      "[41 / 50]   Val: Loss = 0.23201, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 120.20it/s]\n",
      "[42 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.95it/s] \n",
      "[42 / 50]   Val: Loss = 0.23291, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 118.63it/s]\n",
      "[43 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 87.76it/s]\n",
      "[43 / 50]   Val: Loss = 0.23273, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 115.95it/s]\n",
      "[44 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 90.35it/s] \n",
      "[44 / 50]   Val: Loss = 0.23590, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 115.32it/s]\n",
      "[45 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 86.91it/s] \n",
      "[45 / 50]   Val: Loss = 0.23630, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 117.25it/s]\n",
      "[46 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.69it/s]\n",
      "[46 / 50]   Val: Loss = 0.23776, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 124.38it/s]\n",
      "[47 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 89.74it/s] \n",
      "[47 / 50]   Val: Loss = 0.24186, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 115.63it/s]\n",
      "[48 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 92.09it/s] \n",
      "[48 / 50]   Val: Loss = 0.24178, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 122.53it/s]\n",
      "[49 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 85.31it/s]\n",
      "[49 / 50]   Val: Loss = 0.24677, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 114.87it/s]\n",
      "[50 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 91.36it/s]\n",
      "[50 / 50]   Val: Loss = 0.25986, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 128.32it/s]\n"
     ]
    }
   ],
   "source": [
    "class BILSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
    "        self.bilstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out, _ = self.bilstm(self.embedding(inputs))\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "model = BILSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 1] Train: Loss = 0.26220, Accuracy = 96.45%: 100%|██████████| 448/448 [00:01<00:00, 306.32it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(model, criterion, None, train_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embeddings))\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.shape[1], hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
    "        self.linear = nn.Linear(2*lstm_hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out, _ = self.lstm(self.embedding(inputs))\n",
    "        out = self.linear(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EBtI6BDE-Fc7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.56073, Accuracy = 83.77%: 100%|██████████| 572/572 [00:04<00:00, 141.38it/s]\n",
      "[1 / 50]   Val: Loss = 0.25646, Accuracy = 92.42%: 100%|██████████| 13/13 [00:00<00:00, 141.40it/s]\n",
      "[2 / 50] Train: Loss = 0.18824, Accuracy = 94.42%: 100%|██████████| 572/572 [00:04<00:00, 139.57it/s]\n",
      "[2 / 50]   Val: Loss = 0.17398, Accuracy = 94.78%: 100%|██████████| 13/13 [00:00<00:00, 148.33it/s]\n",
      "[3 / 50] Train: Loss = 0.13355, Accuracy = 96.03%: 100%|██████████| 572/572 [00:04<00:00, 138.46it/s]\n",
      "[3 / 50]   Val: Loss = 0.14167, Accuracy = 95.69%: 100%|██████████| 13/13 [00:00<00:00, 146.73it/s]\n",
      "[4 / 50] Train: Loss = 0.10750, Accuracy = 96.74%: 100%|██████████| 572/572 [00:04<00:00, 140.78it/s]\n",
      "[4 / 50]   Val: Loss = 0.12268, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 142.50it/s]\n",
      "[5 / 50] Train: Loss = 0.09210, Accuracy = 97.20%: 100%|██████████| 572/572 [00:04<00:00, 141.04it/s]\n",
      "[5 / 50]   Val: Loss = 0.11329, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 150.19it/s]\n",
      "[6 / 50] Train: Loss = 0.08180, Accuracy = 97.51%: 100%|██████████| 572/572 [00:04<00:00, 141.53it/s]\n",
      "[6 / 50]   Val: Loss = 0.10636, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 140.18it/s]\n",
      "[7 / 50] Train: Loss = 0.07433, Accuracy = 97.71%: 100%|██████████| 572/572 [00:03<00:00, 143.15it/s]\n",
      "[7 / 50]   Val: Loss = 0.10229, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 141.89it/s]\n",
      "[8 / 50] Train: Loss = 0.06837, Accuracy = 97.90%: 100%|██████████| 572/572 [00:04<00:00, 142.89it/s]\n",
      "[8 / 50]   Val: Loss = 0.10062, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 140.74it/s]\n",
      "[9 / 50] Train: Loss = 0.06335, Accuracy = 98.03%: 100%|██████████| 572/572 [00:04<00:00, 142.20it/s]\n",
      "[9 / 50]   Val: Loss = 0.09732, Accuracy = 96.93%: 100%|██████████| 13/13 [00:00<00:00, 139.98it/s]\n",
      "[10 / 50] Train: Loss = 0.05942, Accuracy = 98.17%: 100%|██████████| 572/572 [00:04<00:00, 140.85it/s]\n",
      "[10 / 50]   Val: Loss = 0.09479, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 147.16it/s]\n",
      "[11 / 50] Train: Loss = 0.05574, Accuracy = 98.27%: 100%|██████████| 572/572 [00:03<00:00, 144.46it/s]\n",
      "[11 / 50]   Val: Loss = 0.09462, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 143.05it/s]\n",
      "[12 / 50] Train: Loss = 0.05280, Accuracy = 98.36%: 100%|██████████| 572/572 [00:03<00:00, 144.30it/s]\n",
      "[12 / 50]   Val: Loss = 0.09546, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 157.71it/s]\n",
      "[13 / 50] Train: Loss = 0.04984, Accuracy = 98.45%: 100%|██████████| 572/572 [00:04<00:00, 141.03it/s]\n",
      "[13 / 50]   Val: Loss = 0.09572, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 148.38it/s]\n",
      "[14 / 50] Train: Loss = 0.04742, Accuracy = 98.53%: 100%|██████████| 572/572 [00:04<00:00, 141.58it/s]\n",
      "[14 / 50]   Val: Loss = 0.09491, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 149.65it/s]\n",
      "[15 / 50] Train: Loss = 0.04507, Accuracy = 98.60%: 100%|██████████| 572/572 [00:04<00:00, 142.48it/s]\n",
      "[15 / 50]   Val: Loss = 0.09850, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 147.49it/s]\n",
      "[16 / 50] Train: Loss = 0.04293, Accuracy = 98.67%: 100%|██████████| 572/572 [00:04<00:00, 136.80it/s]\n",
      "[16 / 50]   Val: Loss = 0.09819, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 152.12it/s]\n",
      "[17 / 50] Train: Loss = 0.04107, Accuracy = 98.72%: 100%|██████████| 572/572 [00:03<00:00, 143.63it/s]\n",
      "[17 / 50]   Val: Loss = 0.09623, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 161.91it/s]\n",
      "[18 / 50] Train: Loss = 0.03923, Accuracy = 98.78%: 100%|██████████| 572/572 [00:03<00:00, 145.67it/s]\n",
      "[18 / 50]   Val: Loss = 0.09734, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 151.66it/s]\n",
      "[19 / 50] Train: Loss = 0.03763, Accuracy = 98.84%: 100%|██████████| 572/572 [00:04<00:00, 140.17it/s]\n",
      "[19 / 50]   Val: Loss = 0.09534, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 165.66it/s]\n",
      "[20 / 50] Train: Loss = 0.03569, Accuracy = 98.90%: 100%|██████████| 572/572 [00:03<00:00, 146.77it/s]\n",
      "[20 / 50]   Val: Loss = 0.09966, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 164.49it/s]\n",
      "[21 / 50] Train: Loss = 0.03440, Accuracy = 98.95%: 100%|██████████| 572/572 [00:04<00:00, 142.23it/s]\n",
      "[21 / 50]   Val: Loss = 0.09707, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 148.02it/s]\n",
      "[22 / 50] Train: Loss = 0.03273, Accuracy = 99.00%: 100%|██████████| 572/572 [00:03<00:00, 145.06it/s] \n",
      "[22 / 50]   Val: Loss = 0.10058, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 148.08it/s]\n",
      "[23 / 50] Train: Loss = 0.03132, Accuracy = 99.05%: 100%|██████████| 572/572 [00:03<00:00, 143.02it/s]\n",
      "[23 / 50]   Val: Loss = 0.10380, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 147.58it/s]\n",
      "[24 / 50] Train: Loss = 0.03007, Accuracy = 99.09%: 100%|██████████| 572/572 [00:04<00:00, 138.97it/s]\n",
      "[24 / 50]   Val: Loss = 0.10148, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 150.98it/s]\n",
      "[25 / 50] Train: Loss = 0.02852, Accuracy = 99.14%: 100%|██████████| 572/572 [00:04<00:00, 141.65it/s]\n",
      "[25 / 50]   Val: Loss = 0.10464, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 146.50it/s]\n",
      "[26 / 50] Train: Loss = 0.02738, Accuracy = 99.18%: 100%|██████████| 572/572 [00:04<00:00, 142.58it/s]\n",
      "[26 / 50]   Val: Loss = 0.10557, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 155.46it/s]\n",
      "[27 / 50] Train: Loss = 0.02624, Accuracy = 99.22%: 100%|██████████| 572/572 [00:03<00:00, 145.37it/s]\n",
      "[27 / 50]   Val: Loss = 0.11050, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 166.83it/s]\n",
      "[28 / 50] Train: Loss = 0.02502, Accuracy = 99.25%: 100%|██████████| 572/572 [00:04<00:00, 138.95it/s]\n",
      "[28 / 50]   Val: Loss = 0.11014, Accuracy = 96.95%: 100%|██████████| 13/13 [00:00<00:00, 155.28it/s]\n",
      "[29 / 50] Train: Loss = 0.02379, Accuracy = 99.30%: 100%|██████████| 572/572 [00:04<00:00, 141.54it/s]\n",
      "[29 / 50]   Val: Loss = 0.11136, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 156.54it/s]\n",
      "[30 / 50] Train: Loss = 0.02297, Accuracy = 99.34%: 100%|██████████| 572/572 [00:04<00:00, 140.19it/s] \n",
      "[30 / 50]   Val: Loss = 0.11542, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 137.47it/s]\n",
      "[31 / 50] Train: Loss = 0.02180, Accuracy = 99.36%: 100%|██████████| 572/572 [00:04<00:00, 140.27it/s]\n",
      "[31 / 50]   Val: Loss = 0.11499, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 153.94it/s]\n",
      "[32 / 50] Train: Loss = 0.02064, Accuracy = 99.41%: 100%|██████████| 572/572 [00:04<00:00, 142.12it/s] \n",
      "[32 / 50]   Val: Loss = 0.11743, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 170.25it/s]\n",
      "[33 / 50] Train: Loss = 0.01977, Accuracy = 99.44%: 100%|██████████| 572/572 [00:03<00:00, 146.03it/s]\n",
      "[33 / 50]   Val: Loss = 0.12059, Accuracy = 96.85%: 100%|██████████| 13/13 [00:00<00:00, 146.66it/s]\n",
      "[34 / 50] Train: Loss = 0.01882, Accuracy = 99.47%: 100%|██████████| 572/572 [00:04<00:00, 141.26it/s] \n",
      "[34 / 50]   Val: Loss = 0.12166, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 158.28it/s]\n",
      "[35 / 50] Train: Loss = 0.01809, Accuracy = 99.50%: 100%|██████████| 572/572 [00:04<00:00, 136.78it/s] \n",
      "[35 / 50]   Val: Loss = 0.12244, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 160.09it/s]\n",
      "[36 / 50] Train: Loss = 0.01698, Accuracy = 99.53%: 100%|██████████| 572/572 [00:03<00:00, 145.55it/s] \n",
      "[36 / 50]   Val: Loss = 0.12475, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 154.42it/s]\n",
      "[37 / 50] Train: Loss = 0.01622, Accuracy = 99.56%: 100%|██████████| 572/572 [00:04<00:00, 136.32it/s] \n",
      "[37 / 50]   Val: Loss = 0.12731, Accuracy = 96.79%: 100%|██████████| 13/13 [00:00<00:00, 151.92it/s]\n",
      "[38 / 50] Train: Loss = 0.01534, Accuracy = 99.60%: 100%|██████████| 572/572 [00:04<00:00, 130.29it/s] \n",
      "[38 / 50]   Val: Loss = 0.13129, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 149.70it/s]\n",
      "[39 / 50] Train: Loss = 0.01463, Accuracy = 99.62%: 100%|██████████| 572/572 [00:04<00:00, 136.37it/s] \n",
      "[39 / 50]   Val: Loss = 0.13120, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 150.78it/s]\n",
      "[40 / 50] Train: Loss = 0.01380, Accuracy = 99.64%: 100%|██████████| 572/572 [00:04<00:00, 137.14it/s] \n",
      "[40 / 50]   Val: Loss = 0.13645, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 141.17it/s]\n",
      "[41 / 50] Train: Loss = 0.01311, Accuracy = 99.67%: 100%|██████████| 572/572 [00:04<00:00, 136.52it/s] \n",
      "[41 / 50]   Val: Loss = 0.13788, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 143.93it/s]\n",
      "[42 / 50] Train: Loss = 0.01236, Accuracy = 99.68%: 100%|██████████| 572/572 [00:04<00:00, 136.11it/s] \n",
      "[42 / 50]   Val: Loss = 0.13990, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 164.13it/s]\n",
      "[43 / 50] Train: Loss = 0.01182, Accuracy = 99.71%: 100%|██████████| 572/572 [00:04<00:00, 142.34it/s] \n",
      "[43 / 50]   Val: Loss = 0.14211, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 137.88it/s]\n",
      "[44 / 50] Train: Loss = 0.01115, Accuracy = 99.73%: 100%|██████████| 572/572 [00:04<00:00, 136.96it/s] \n",
      "[44 / 50]   Val: Loss = 0.14413, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 148.97it/s]\n",
      "[45 / 50] Train: Loss = 0.01049, Accuracy = 99.76%: 100%|██████████| 572/572 [00:04<00:00, 135.24it/s] \n",
      "[45 / 50]   Val: Loss = 0.14993, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 157.30it/s]\n",
      "[46 / 50] Train: Loss = 0.00993, Accuracy = 99.77%: 100%|██████████| 572/572 [00:04<00:00, 137.52it/s] \n",
      "[46 / 50]   Val: Loss = 0.14844, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 157.48it/s]\n",
      "[47 / 50] Train: Loss = 0.00925, Accuracy = 99.79%: 100%|██████████| 572/572 [00:04<00:00, 136.60it/s] \n",
      "[47 / 50]   Val: Loss = 0.15197, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 143.98it/s]\n",
      "[48 / 50] Train: Loss = 0.00880, Accuracy = 99.80%: 100%|██████████| 572/572 [00:04<00:00, 134.47it/s] \n",
      "[48 / 50]   Val: Loss = 0.15727, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 146.97it/s]\n",
      "[49 / 50] Train: Loss = 0.00857, Accuracy = 99.81%: 100%|██████████| 572/572 [00:04<00:00, 127.14it/s] \n",
      "[49 / 50]   Val: Loss = 0.16048, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 129.96it/s]\n",
      "[50 / 50] Train: Loss = 0.00777, Accuracy = 99.84%: 100%|██████████| 572/572 [00:04<00:00, 130.33it/s] \n",
      "[50 / 50]   Val: Loss = 0.16232, Accuracy = 96.59%: 100%|██████████| 13/13 [00:00<00:00, 159.64it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
    "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512, train_hstr=train_history, val_hstr=val_history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 1] Train: Loss = 0.16288, Accuracy = 96.70%: 100%|██████████| 448/448 [00:01<00:00, 343.64it/s]\n"
     ]
    }
   ],
   "source": [
    "fit(model, criterion, None, train_data=(X_test, y_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)\n",
    "plt.gca().legend(('train_hstr', 'val_hstr'))\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}